* #monitoringsucks
It sure used to.

I remember using Nagios and cfengine to configure it.

Experimented with cacti.

Now I see collectd, graphite, and a world of new tools.
#monitoringsucks
What to talk about?

I have no idea what has gone on in the real world since then.

These #monitoringsucks people seem to be on the right path... do I have
anything new to add?



I don't remember the first time I saw "#monitoringsucks" on Twitter, but it's
been a growing theme since then with several devops luminaries all chiming in
with their 70 characters/cent.

I can appreciate the sentiment.  I once used Nagios and tried to scale it.

I set up cfengine to configure all the checks on all the hosts, and then wrote
a templating language for cfengine to make it scale.

however I can't agree that monitoring sucks.   Monitoring is awesome!


* Validation
http://blog.lusis.org/blog/2012/06/05/monitoring-sucking-just-a-little-bit-less/}

"Instead of alerting on data and then storing it as an afterthought (perfdata anyone?) let’s start collecting the
data, storing it and then alerting based on it."


I am glad to have read this!


* What is monitoring?
Measuring,
Recording,
Alerting,
Visualising

* Monitoring systems
automate the boring parts
Measuring,
Recording,
Alerting,
Visualisation

so you have more time to do fun things... and debug the occasional emergency.


Already in our systems, we've automated the taking and recording of
measurements.  For example we have rrdtool and the tools built on top of that,
collectd, we have graphite, we have nagios' performance data extension to the
plugin protocol, and we have third party applications in the cloud offering to record
measurements.

The current state of monitoring:
The alerter
Measure, check against a threshold, fire an alert if check fails.

Measure
Record
Alert
Visualise


For example:
cucumber
nagios
icinga
OpenNMS

The current state of monitoring:
The rear view mirror
Measure, record into a timeseries database, show pretty charts.

Measure
Record
Alert
Visualise
For example:
MRTG
Cacti
ntop
collectd
Ganglia
Munin




* Blackbox vs Whitebox
** Blackbox
Blackbox: treat the system as opaque: you can only use it as a user would.

a.k.a. "probing"

c.f. cucumber-nagios

Flicking a lightbulb on to test you've installed it properly;


The "play a test sound" in the sound control panels on a computer or on your
home theatre system;


"print a test page" on a printer;


trying to open the door after you've
just locked it to make sure it actually locked;


maybe pressing the "lock car"
button on your car key again, just in case, because you can't remember if you
pressed it or if it worked but this time you look for the light flash...


pretty much any everyday use of an object where you are not doing it for the
actual outcome, just to test its working OK.


"It looks a lot like just using the object."

** Whitebox
Whitebox: expose the internal state of the system for inspection

a.k.a. instrumentation, telemetry,
... ROCKET SCIENCE

c.f. ... graphite? new relic? metrics?
Stovetop: clock

Car dashboard: speedometer, tachometer, odometer, tripmeter, clock, fuel gauge
What's wrong with blackbox?
Only boolean: no visibility into why

Why is the site slow?
Why has image serving stopped working?

No predictive capability

How long until we need more disks? cpus? datacenters?

You know you need to get more fuel, but how long can you drive to get there?

I use the tripmeter to give me an estimate...
Pseudo-whitebox
Expose some internal state
Test the latest point in time against a threshold.
Fire an alert.



... ~same as probing
I assert that this is gives you the same information as the blackbox probe --
you don't get the internal state as a result, only the truthiness of the test.




*
Why does monitoring suck?  When the cost of maintenance is too high.

*

I want to take this a step furth

The design of nagios has been around for years




[nagios model]

This is a model of the nagios check/alert paradigm.  Each instance of a check
script is responsible for deciding if there is an alerting condition.  Each
script, or each invocation of the script, needs to pass around the thresholds
to make those alerting decisions.

[tsdb model]

Nowadays, I'm glad to see it, there's a lot of time series databases deployed.
graphite, opentsdb, collectd, statsd, there's a growing number of choices.
This is good.

Nagios' timeseries afterthought, perfdata, gets written into the timeseries
database, and then we generate pretty charts.  Is that all that a timeseries
database is good for though?  Do we only want to use it as a tool for humans,
a systems rear view mirror?

[tsdb and app stats]

I should take a small moment to acknowledge the advent of application metrics
going into timeseries databases.  Some people call this real user monitoring.
I call it merely whitebox monitoring, the opposite of the blackbox check; in
the latter case you only know the interface, and that's how you test that it's
working, in the former you get to see inside the box and look at the state of
the application.

[+check-graphite]

and once you have the application state in a timeseries database, you decide
you don't want run check\_http on your application backends, and point it right
back at the timeseries database.

The problem I see is that you still have to run a check script for every metric
you want to test against.  So you have a web scale application and thousands of
timeseries, and a check script for each.  Or perhaps you are smart and can run
one check script across a class of timeseries.

Well, I think the idea of the check script is still bogus.


Problems with the check/alert model
Thresholds vary among instances, tuning difficult.
Adding new targets, new checks is lots of effort.
Checking logic performs the measurement and the "judgement" all in one.
Alerts for things you can't act on.
Application health
Self service

I claim that this doesn't scale well for maintenance costs and monitoring
resource costs.  Even if you have written tools to autogenerate your nagios
configuration.  The cost of adding a new metric to an existing check script, or
adding a new invocation of a check script for a different class of timeseries,
or adding a new host, or regenerating new monitoring for existing hosts, all
carries a non trivial amount of work, and along with that a non trivial amount
of risk.

Let's also not forget that every check script incurs a physical cost on the
monitoring infrastructure.  Christian from Anchor will be talking later this
conference about scaling Nagios; I suppose we will have to fight later.


Hard to configure and doesn't scale well.
An idea...
http://blog.lusis.org/blog/2012/06/05/monitoring-sucking-just-a-little-bit-less/

"Instead of alerting on data and then storing it as an afterthought (perfdata
anyone?) let’s start collecting the data, storing it and then alerting based on
it."


Need something more generic - more scalable?

[new design]

So how do you make this scale?

Nagios does two things; collect metrics, and test them against a set of rules.
The rules map 1:1 to the metrics, embodied in the instance of a script.

So, we can start by separating the collection from the rules.  We still need to
collect metrics from every target; but we don't have to run N scripts to
collect N metrics; we can collect N metrics in a single scrape of a target.

We don't need 1 instance of a rule for every metric; we can reuse the same rule
across many metrics.  If you treat the metrics as a vector, you can apply
vector maths to it.  Get me all the hosts that have a load higher than 10 the
last time I scraped, it's just filtering a vector.


Back to reality; we can do this currently by abusing nagios' perfdata to write
large gloms of data into a timeseries database; or not use nagios at all and
write straight to the tsdb; and then write check scripts that know how to apply
a rule to a class of timeseries.

I think I mentioned that mode already, though.

So let's go on a tangent now and think about the kinds of things you can do
when you ignore nagios and the check script, and think solely about alerting
based on the timeseries themselves.





* Alerting on thresholds

Some real world examples; this one is a good match for the simple check/alert model.

%% https://www.youtube.com/watch?v=kn_dYZn5TEQ&feature=player_embedded

** Alert when beer supply low

if cases - 1 - 1 <= 1:
  alert Barney Worried About Beer Supply

** Disk full alert
Alert when 90% full
Different filesystems have different sizes
10% of 2TB is 200GB
False positive!

Alert on absolute space, < 500MB
Arbitrary number
Different workloads with different needs 500MB might not be enough warning
Some alerts don't map well to the check/alert model.
Disk full alert
More generic alert:
How long before the disk is full?

How long will it take to respond to an (almost) full disk?


** Alerting on rates of change

%% src=https://www.youtube.com/embed/pfwmMfyPCB8?rel=0&start=71&end=85&autoplay=1

(skip to 1:11)

More complex real world example.

Dennis Hopper's alert

foreach speed any time in the past:
  if speed > 50mph:
	alert Bomb Armed

if Bomb Armed and speed < 50mph:
  ...

** Keanu's alert
if speed > 50mph:
  alert Save the bus!

First he wants to know what bus it is.

* Keanu's alert

it doesn't matter how long it takes to save thew bus as long as the bus stays
above 50mph. At what point does the rescue operation become time critical?

When the bus starts slowing down, acceleration goes negative.

** Keanu's alert

speed - acceleration * time = 50

50 - speed = - acceleration * time

(speed - 50)/acceleration = time


if (speed - 50)/acceleration <= time to save bus:
  alert Start saving the bus!


You don't necessarily care that acceleration is negative though -- but when it
is, you do want to know if it's rapidly decelerating, because that changes how
much time you have to finish the rescue.

* New tools at our disposal

Calculus!

the derivative of speed = acceleration

the derivative of acceleration = ... jerk

(impulse?)


So timeseries are just curves.  We can apply high school calculus to them!

There's a few caveats, though; look close enough and your timeseries are
discreet.

** Error spike
error count

In this example, our errors per second rises, but we have no threshold. Perhaps
your threshold is 0? What about user generated errors, like 404s?

** Rate of errors vs normal rate
rate of change increases greater than expected

** errors per second
Now we're looking at the rate of errors, we can set the threshold for alerting
based on the noise floor of errors.

calculate rate of change of timeseries

**
Summary: New tool #1: calculate the rates of things and compare the rate
against a threshold.


** Another new tool
Not just looking at the latest data point, or the derivative at the latest
point

Look back 5 minutes, 1 hour, 7 days, back to the dawn of time



The next new tool alluded to in the Speed example is historical data. We have a
whole time*series* available, so we don't have to limit our checks to only the
most recent data point. I know some checks store the last few data points for
calculating rates or trends over the short term. How many data points is
enough? With the timeseries database, you don't have to answer that question at
the check level.

** Traffic spike with threshold
worth getting out of bed for?

Let's say this is a rate of errors which we just started calculating. Do you
want to be paged for a problem that seems to have subsided?

Sure you might want to know about this issue for debugging, but was it
necessary to wake you at 3am to tell you about it?
 Δt

** Traffic spike with threshold
worth getting out of bed for?

NO

maybe?

Δt

When an alerting condition arrives, why not wait for a bit and see if the
condition is stable. You do add a bit of latency to the alert (delta t at least
before the alert fires) but you reduce your false positive rate and keep the
oncall operator a little bit more sane, and their spouse happy!

Consider also flapping alerts.
observe timeseries history to gain context

**
Summary: New tool #2: Historical analysis.

* Timeseries Have Types
** Counter
Counter: monotonically nondecreasing
  "preserves the order" i.e. UP
  "nondecreasing" can be flat

So there's a few properties of timeseries that makes classifying them useful.

A counter indicates discreet events like the number of units of a measurand an
 action has been taken on the number of queries received, number of bytes
 transferred.


A counter can only indicate a base unit, such as distance, or time, or count of
queries, since a reference point.

** Gauge
Gauge: everything else... not monotonic


a gauge indicates a point in time, like a quantity of fuel remaining, a
length of a queue, or velocity.

A gauge can indicate derived units, such as revolutions per second, as well as
base units such as litres.


The difference is a counter is always increasing, whereas a gauge indicates a
measurand increase or decrease over time.



** Counters FTW
Δt

Alas we cannot store every point in a timeseries; even though we said they're
discreet so we don't need to store infinity points, we probably are still
memory constrained, or CPU constrained, which affects the physical ability to
collect the timeseries. Depending on your requirements, perhaps you only need a
point every minute or so.

The counter preserves information despite the loss of data by downsampling.  We
know just how much a counter has increased since the last time we measured it,
because we can guarantee that the counter hasn't increased any more than the
value we've seen.

** Counters FTW
no loss of meaning after sampling
Δt

** Gauges FTL
Δt

The gauge, on the other hand, has no regard for the sampling interval, and will
happily spike while you're not looking.

** Gauges FTL
lose spike events shorter than sampling interval

Δt

So it's better in general to export your data as a counter, and perform your
calculations on that base unit.

prefer counters over gauges

**
Gauges lose data when sampling.
Counters do not.

Prefer counters over gauges as base unit single dimension, easier to work with
(and no missed spikes!)

* Another new tool
Instances in a cluster don't work alone.

Discover the properties of the system by aggregating the parts.

How many queries per second is your cluster receiving?

Sum the query counters across the cluster, and calculate the rate!

** Aggregation

cluster rate = rate(instance 1 + instance 2)

Timeseries are lists of points. Assuming the sampling rate is the same along
all of them, you can sum elements at the same position in each list together.
aggregate to each logical grouping in the system

* high school maths recap
** Timeseries Operations: Rates

δ(counter)/δt = gauge

δ(gauge)/δt = gauge

(beware of sampling errors)

You change types of timeseries when applying some operations, for example
taking the rate of a counter turns it into a gauge. This is not bad as long as
you remember to keep the counter around, as we'll see.

As an approximation, you can precalculate the rate at the end of each sampling
interval by taking the delta between this and the last data point.

But beware of sampling errors; if you miss a collection, then there's no data
to compare to. Do you want to take the rate to the last valid data point if it
was 10 minutes ago?

** Timeseries Operations: Aggregation
Σ0..n(counter) = counter
Σ0..n(gauge) = gauge


(beware of sampling error and quantization)

To save time, precompute the sum just after each sample interval.

Remember the timeseries is actually discreet points in time; two timeseries
probably don't align up on timestamps exactly.

** Timeseries Operations: Ratios
counter / counter = counter: instant means
gauge / gauge = gauge: rate comparisons

e.g. New deployment
δ(errors) / δ(queries) > threshold?

Is the rate of rate of errors over rate of queries too high? :-)

Apply historical analysis again; instead of hardcoding a threshold, how about
the mean error/query ratio rate for the lifetime of the last deployment?

** Histograms

** Timeseries Operations: Mean
gauge = gauge*
counter = counter


What sort of mean?
Mean over N datapoints in single timeseries
Mean over N instances of same timeseries

Treat the group timeseries as a list of vectors; are you taking the mean along
a single vector or along the same column?

Mean aggregations
Zone 1 has 5 tasks, zone 2 has 10
Both doing 100 qps

Mean 10 minute query rate?
Zone 1: 100/5 = 20
Zone 2: 100/10 = 10

Is the global average 15 qps per job?
Incorrect Mean
level 1 sum = sum(counters)
level 1 count = len(counters)
level 1 mean = level 1 sum / level 1 count

level 2 mean = sum(level 1 mean)/ len(level 1 mean)

(20 + 10)  / 2 = 15
The wrong way.
Correct Mean
level 1 sum = sum(counters)
level 1 count = len(counters)
level 1 mean = level 1 sum / level 1 count

level 2 sum = sum(level 1 sum)
level 2 count = sum(level 1 count)
level 2 mean = level 2 sum / level 2 count

(100 + 100) / (10 + 5) = 13.3

keep sums and counts at each level to prevent error
Sum counters before calculating rates at an aggregation level

Mean only defined over same denominator, keep count and sum at each aggregation
level

** What to measure?
All well and good, but you want me to give you concrete examples.

** What to measure?
Not definitive, but a good place to start:

Queries per second
What is a query?

Errors per second
Type of error

Latency
by query type, response code, payload size

Bandwidth
by direction, query type, response code, ...

** What not to measure
Load average..

** What to alert on?
Rate of change of QPS outside normal cycles
Ratio of errors to queries
Latency (mean, 95th percentile) too high
Rate of change of bandwidth

Make sure it's ACTIONABLE...

then DOCUMENT IT


** Caveats with timeseries based alerting
Timeseries alerting is a huge space

unlimited number of ways to break system

amount of logic necessary for high coverage is staggering

false positive rate must be low

alert tuning is time consuming

alert logic must be simple

Blackbox testing still necessary

End-to-end testing by definition covers everything you have missed,

you still have charts in the timeseries to inspect, right?

** TLDL
Do maths on your timeseries (sums, rates)
Keep counters instead of gauges, derive rates
Compare them to one another (ratios)
Do historical analysis (compare values over time)
Alert only when action can be taken


* What next?
Attach a statistical package to your timeseries database, and experiment

R
numpy
Processing
your favourite here

* Make smarter alerts!

I've talked about some of the mathematical techniques for analysing timeseries
to be smarter about alerting

I can't give you the software we use internally, but

Here's some statistical tools you can use to start analysing your own data

If you can hook them up to your alerting system, then fantastic! I encourage
you all to experiment and hopefully come up with the next generation of
monitoring tools!

Don't just stick to the operations I've suggested -- there's many statistical
methods at your disposal I haven't mentioned, including percentiles and
distribution functions, scatterplots and line fitting.

Any Questions!?
